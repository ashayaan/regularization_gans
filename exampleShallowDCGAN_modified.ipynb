{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "import itertools, imageio, pickle\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import scipy\n",
    "import numpy as np\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "1.12.0\n",
      "--------------------------\n",
      "/home/shayaan/Columbia/sem_1/Deep-Learning-for-OR-and-FE/project\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "print(tf.executing_eagerly())\n",
    "print(tf.test.is_gpu_available())\n",
    "print(tf.__version__)\n",
    "\n",
    "sizePixel1 = 512\n",
    "sizePixel2 = sizePixel1*sizePixel1\n",
    "\n",
    "size_d = 32\n",
    "\n",
    "batch_size = 22\n",
    "train_epoch = 3\n",
    "\n",
    "folder = '/home/shayaan/Columbia/sem_1/Deep-Learning-for-OR-and-FE/project/png'\n",
    "\n",
    "pngLocation = 'Fixed_results'\n",
    "\n",
    "root = 'outputConGANs/'\n",
    "model = 'DCGAN_con_run3_'\n",
    "\n",
    "# Though it's not possible to get the path to the notebook by __file__, os.path is still very useful in dealing with paths and files\n",
    "# In this case, we can use an alternative: pathlib.Path\n",
    "\"\"\"\n",
    "code_dir   = os.path.dirname(__file__)\n",
    "\"\"\"\n",
    "#get the current path of our code\n",
    "\n",
    "code_dir = '/home/shayaan/Columbia/sem_1/Deep-Learning-for-OR-and-FE/project'\n",
    "print(\"--------------------------\")\n",
    "code_dir\n",
    "print(code_dir)\n",
    "print(\"--------------------------\")\n",
    "#create output_dir within the same path\n",
    "output_dir = code_dir + 'outputConGANs/'\n",
    "\n",
    "\n",
    "if not os.path.isdir(output_dir):\n",
    "    os.makedirs(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tensor_from_image_file(path, input_height=sizePixel1, input_width=sizePixel1, input_mean=0, input_std=255):\n",
    "    \n",
    "    input_name = \"file_reader\"\n",
    "    output_name = \"normalized\"\n",
    "    file_reader = tf.read_file(path, input_name)\n",
    "    image_reader = tf.image.decode_png(file_reader, channels = 1)\n",
    "    float_caster = tf.cast(image_reader, tf.float32)\n",
    "    dims_expander = tf.expand_dims(float_caster, 0);\n",
    "    resized = tf.image.resize_bilinear(dims_expander, [input_height, input_width])\n",
    "    normalized = tf.divide(tf.subtract(resized, [input_mean]), [input_std])\n",
    "    sess = tf.Session()\n",
    "    result = sess.run(normalized)\n",
    "    sess.close()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nSeries = 222-220\n",
    "numInSeries = 11\n",
    "nImages = nSeries*numInSeries #2442\n",
    "img  = np.zeros((nImages, sizePixel1,sizePixel1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 1, 0)\n",
      "(1, 2, 0)\n",
      "(2, 1, 1)\n",
      "(3, 2, 1)\n",
      "(4, 1, 2)\n",
      "(5, 2, 2)\n",
      "(6, 1, 3)\n",
      "(7, 2, 3)\n",
      "(8, 1, 4)\n",
      "(9, 2, 4)\n",
      "(10, 1, 5)\n",
      "(11, 2, 5)\n",
      "(12, 1, 6)\n",
      "(13, 2, 6)\n",
      "(14, 1, 7)\n",
      "(15, 2, 7)\n",
      "(16, 1, 8)\n",
      "(17, 2, 8)\n",
      "(18, 1, 9)\n",
      "(19, 2, 9)\n",
      "(20, 1, 10)\n",
      "(21, 2, 10)\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for j in range(0,numInSeries):\n",
    "    for i in range(1,nSeries+1):\n",
    "        print(counter,i,j)\n",
    "        fname = str(i) + '_' + str(j) + '.png'\n",
    "        path = folder + '/' + fname\n",
    "        orig_img = read_tensor_from_image_file(path)\n",
    "        # vectorize\n",
    "        #img[counter] = orig_img.reshape(-1)\n",
    "        \n",
    "        # original size\n",
    "        img[counter] = orig_img.reshape(sizePixel1,sizePixel1)\n",
    "        counter = counter+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lrelu(x, th=0.2):\n",
    "    return tf.maximum(th * x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_score_np(imgSample):\n",
    "    \n",
    "    tmp1 = imgSample.copy()\n",
    "    #tmp1 = tmp1.reshape(512,512)\n",
    "    #tmp2 = tmp1\n",
    "    min1 = tmp1.min()\n",
    "    max1 = tmp1.max()\n",
    "    \n",
    "    tmp1[tmp1< 0.5]=0.0\n",
    "    tmp1[tmp1>=0.5]=1.0 \n",
    "    #print(min1)\n",
    "    #print(max1)\n",
    "    tmp1 = 1.0-tmp1\n",
    "    label_im, nb_labels = scipy.ndimage.label(tmp1)\n",
    "    \n",
    "    #print(nb_labels)\n",
    "    \n",
    "    #plt.subplot(121)\n",
    "    #plt.imshow(tmp1)\n",
    "    \n",
    "    #plt.subplot(122)\n",
    "    #plt.imshow(tmp2)\n",
    "\n",
    "    return float(nb_labels), min1, max1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(x, isTrain=True, reuse=False):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        \n",
    "#         print('')\n",
    "#         print('inside generator')\n",
    "#         print('================')\n",
    "#         print('x')\n",
    "#         print(x.get_shape())\n",
    "\n",
    "        # 1st hidden layer\n",
    "        conv1 = tf.layers.conv2d_transpose(x, size_d*2*2*2*2*2*2, [2, 2], strides=(1, 1), padding='valid')\n",
    "        lrelu1 = lrelu(tf.layers.batch_normalization(conv1, training=isTrain), 0.2)\n",
    "        lrelu1 = tf.clip_by_value(lrelu1, clip_value_min=0.5, clip_value_max=1)\n",
    "        lrelu1 = 1 - lrelu1\n",
    "        # score_of_layer_1 =  compute_score_np(lrelu1.eval())\n",
    "        # print (score_of_layer_1)\n",
    "#         print('conv1')\n",
    "#         print(conv1.get_shape())\n",
    "        \n",
    "        # 2nd hidden layer\n",
    "        conv2 = tf.layers.conv2d_transpose(lrelu1, size_d*2*2*2*2, [7, 7], strides=(4, 4), padding='same')\n",
    "        lrelu2 = lrelu(tf.layers.batch_normalization(conv2, training=isTrain), 0.2)\n",
    "        lrelu2 = tf.clip_by_value(lrelu2, clip_value_min=0.5, clip_value_max=1)\n",
    "        lrelu2 = 1 - lrelu2\n",
    "#         print('conv2')\n",
    "#         print(conv2.get_shape())\n",
    "        \n",
    "        # 3rd hidden layer\n",
    "        conv3 = tf.layers.conv2d_transpose(lrelu2, size_d*2*2, [7, 7], strides=(4, 4), padding='same')\n",
    "        lrelu3 = lrelu(tf.layers.batch_normalization(conv3, training=isTrain), 0.2)\n",
    "        lrelu3 = tf.clip_by_value(lrelu3, clip_value_min=0.5, clip_value_max=1)\n",
    "        lrelu3 = 1 - lrelu3\n",
    "#         print('conv3')\n",
    "#         print(conv3.get_shape())\n",
    "        \n",
    "        # 7th hidden layer\n",
    "        conv4 = tf.layers.conv2d_transpose(lrelu3, size_d, [7, 7], strides=(4, 4), padding='same')\n",
    "        lrelu4 = lrelu(tf.layers.batch_normalization(conv4, training=isTrain), 0.2)\n",
    "        lrelu4 = tf.clip_by_value(lrelu4, clip_value_min=0.5, clip_value_max=1)\n",
    "        lrelu4 = 1 - lrelu4\n",
    "#         print('conv4')\n",
    "#         print(conv4.get_shape())\n",
    "        \n",
    "        # output layer\n",
    "        conv5 = tf.layers.conv2d_transpose(lrelu4, 1, [7, 7], strides=(4, 4), padding='same')\n",
    "        #o = tf.nn.tanh(conv5)\n",
    "        o = tf.nn.sigmoid(conv5)\n",
    "        print (o)\n",
    "        \n",
    "#         print('conv5')\n",
    "#         print(conv5.get_shape())\n",
    "#         print(o.get_shape())\n",
    "#         print('-----------------')\n",
    "\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(x, isTrain=True, reuse=False):\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        \n",
    "        print('')\n",
    "        print('inside discriminator')\n",
    "        print('====================')\n",
    "        print('x')\n",
    "        print(x.get_shape())\n",
    "        \n",
    "        # 1st hidden layer\n",
    "        conv1 = tf.layers.conv2d(x, size_d, [7, 7], strides=(4, 4), padding='same')\n",
    "\n",
    "        lrelu1 = lrelu(conv1, 0.2)\n",
    "        \n",
    "#         print('conv1')\n",
    "#         print(conv1.get_shape())\n",
    "\n",
    "        # 2nd hidden layer\n",
    "        conv2 = tf.layers.conv2d(lrelu1, size_d*2*2, [7, 7], strides=(4, 4), padding='same')\n",
    "        lrelu2 = lrelu(tf.layers.batch_normalization(conv2, training=isTrain), 0.2)\n",
    "#         print('conv2')\n",
    "#         print(conv2.get_shape())\n",
    "\n",
    "        # 3rd hidden layer\n",
    "        conv3 = tf.layers.conv2d(lrelu2, size_d*2*2*2*2, [7, 7], strides=(4, 4), padding='same')\n",
    "        lrelu3 = lrelu(tf.layers.batch_normalization(conv3, training=isTrain), 0.2)\n",
    "#         print('conv3')\n",
    "#         print(conv3.get_shape())\n",
    "        \n",
    "        # 4th hidden layer\n",
    "        conv4 = tf.layers.conv2d(lrelu3, size_d*2*2*2*2*2*2, [7, 7], strides=(4, 4), padding='same')\n",
    "        lrelu4 = lrelu(tf.layers.batch_normalization(conv4, training=isTrain), 0.2)\n",
    "#         print('conv4')\n",
    "#         print(conv4.get_shape())\n",
    "        \n",
    "        # output layer\n",
    "        conv5 = tf.layers.conv2d(lrelu4, 1, [2, 2], strides=(1, 1), padding='valid')\n",
    "        o = tf.nn.sigmoid(conv5)\n",
    "#         print('conv5')\n",
    "#         print(conv5.get_shape())\n",
    "#         print(o.get_shape())\n",
    "#         print('-----------------')\n",
    "        return o, conv5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sample(samples, size1, size2):\n",
    "    \n",
    "    fig1 = plt.figure(figsize=(size1, size2))\n",
    "    gs = gridspec.GridSpec(size1, size2)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(sample.reshape(512, 512), cmap='gray')\n",
    "\n",
    "    return fig1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_z_ = np.random.normal(0, 1, (50, 1, 1, 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_result(num_epoch, show = False, save = False, path = 'result.png'):\n",
    "    \n",
    "    \n",
    "    test_images = sess.run(G_z, {z: fixed_z_, isTrain: False})\n",
    "\n",
    "    size_figure_grid = 2\n",
    "    fig, ax = plt.subplots(size_figure_grid, size_figure_grid, figsize = (5, 5))\n",
    "    for i, j in itertools.product(range(size_figure_grid), range(size_figure_grid)):\n",
    "        ax[i, j].get_xaxis().set_visible(False)\n",
    "        ax[i, j].get_yaxis().set_visible(False)\n",
    "\n",
    "    for k in range(size_figure_grid*size_figure_grid):\n",
    "        i = k // size_figure_grid\n",
    "        j = k % size_figure_grid\n",
    "        ax[i, j].cla()\n",
    "        ax[i, j].imshow(np.reshape(test_images[k], (512, 512)), cmap='gray')\n",
    "\n",
    "    label = 'Epoch {0}'.format(num_epoch)\n",
    "    fig.text(0.5, 0.04, label, ha='center')\n",
    "\n",
    "    if save:\n",
    "        plt.savefig(path)\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_train_hist(hist, show = False, save = False, path = 'Train_hist_con.png'):\n",
    "    x = range(len(hist['D_losses']))\n",
    "\n",
    "    y1 = hist['D_losses']\n",
    "    y2 = hist['G_losses']\n",
    "\n",
    "    plt.plot(x, y1, label='D_loss')\n",
    "    plt.plot(x, y2, label='G_loss')\n",
    "\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    plt.legend(loc=4)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save:\n",
    "        plt.savefig(path)\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(data, num):\n",
    "    \n",
    "    '''\n",
    "    Return a total of `num` random samples \n",
    "    '''\n",
    "    \n",
    "    #print(len(data))\n",
    "    \n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = array([data[ i] for i in idx])\n",
    "\n",
    "    return data_shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"generator/Sigmoid:0\", shape=(?, 512, 512, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "lr = 0.0002\n",
    "\n",
    "# variables : input\n",
    "x = tf.placeholder(tf.float32, shape=(None, 512, 512, 1))\n",
    "z = tf.placeholder(tf.float32, shape=(None, 1, 1, 100))\n",
    "isTrain = tf.placeholder(dtype=tf.bool)\n",
    "lda = tf.placeholder(tf.float32)\n",
    "\n",
    "g_z = tf.placeholder(tf.float32, shape=(512, 512, 1))\n",
    "\n",
    "G_z = generator(z, isTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "inside discriminator\n",
      "====================\n",
      "x\n",
      "(?, 512, 512, 1)\n",
      "\n",
      "inside discriminator\n",
      "====================\n",
      "x\n",
      "(?, 512, 512, 1)\n"
     ]
    }
   ],
   "source": [
    "D_real, D_real_logits = discriminator(x, isTrain)\n",
    "D_fake, D_fake_logits = discriminator(G_z, isTrain, reuse=True)\n",
    "\n",
    "# loss for each network\n",
    "D_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_real_logits, labels=tf.ones([batch_size, 1, 1, 1])))\n",
    "D_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake_logits, labels=tf.zeros([batch_size, 1, 1, 1])))\n",
    "D_loss = D_loss_real + D_loss_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "minValues = []\n",
    "maxValues = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape before\n",
      "(22, 512, 512, 1)\n",
      "shape after\n",
      "(22, 512, 512, 1)\n"
     ]
    }
   ],
   "source": [
    "scores_1 = tf.placeholder(tf.float32, [None, 1])\n",
    "mins_1 = tf.placeholder(tf.float32, [None, 1])\n",
    "maxs_1 = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake_logits, labels=tf.ones([batch_size, 1, 1, 1])))\n",
    "scores_G = tf.reduce_mean(scores_1)\n",
    "mins_G = tf.reduce_mean(mins_1)\n",
    "maxs_G = tf.reduce_mean(maxs_1)\n",
    "G_loss = G_loss + lda*scores_G\n",
    "\n",
    "# trainable variables for each network\n",
    "T_vars = tf.trainable_variables()\n",
    "D_vars = [var for var in T_vars if var.name.startswith('discriminator')]\n",
    "G_vars = [var for var in T_vars if var.name.startswith('generator')]\n",
    "\n",
    "# optimizer for each network\n",
    "with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "    D_optim = tf.train.AdamOptimizer(lr, beta1=0.5).minimize(D_loss, var_list=D_vars)\n",
    "    G_optim = tf.train.AdamOptimizer(lr, beta1=0.5).minimize(G_loss, var_list=G_vars)\n",
    "\n",
    "# open session and initialize all variables\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "# MNIST resize and normalization\n",
    "#train_set = tf.reshape(img, [sizePixel1,sizePixel1])\n",
    "train_set = np.array(img).reshape(nImages, sizePixel1, sizePixel1, 1)\n",
    "print('shape before')\n",
    "print(train_set.shape)\n",
    "#train_set = (train_set - 0.5) / 0.5  # normalization; range: -1 ~ 1\n",
    "print('shape after')\n",
    "print(train_set.shape)\n",
    "# results save folder\n",
    "\n",
    "if not os.path.isdir(root):\n",
    "    os.mkdir(root)\n",
    "if not os.path.isdir(root + pngLocation):\n",
    "    os.mkdir(root + pngLocation)\n",
    "\n",
    "train_hist = {}\n",
    "train_hist['D_losses'] = []\n",
    "train_hist['G_losses'] = []\n",
    "train_hist['per_epoch_ptimes'] = []\n",
    "train_hist['total_ptime'] = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training start!\n"
     ]
    }
   ],
   "source": [
    "x_ = next_batch(train_set, batch_size)\n",
    "\n",
    "# print('shape')\n",
    "# print(x_.shape)\n",
    "\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "\n",
    "# training-loop\n",
    "np.random.seed(int(time.time()))\n",
    "print('training start!')\n",
    "start_time = time.time()\n",
    "\n",
    "max_iter = 2\n",
    "for epoch in range(train_epoch):\n",
    "    G_losses = []\n",
    "    G_scores = []\n",
    "    G_mins = []\n",
    "    G_maxs = []\n",
    "    D_losses = []\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    if epoch%2 == 0:\n",
    "        lda_ = 1.0\n",
    "    else:\n",
    "        lda_ = 1.0\n",
    "    \n",
    "    for iter in range(max_iter):\n",
    "\n",
    "            \n",
    "        # update discriminator\n",
    "        x_ = next_batch(train_set, batch_size)\n",
    "        z_ = np.random.normal(0, 1, (batch_size, 1, 1, 100))\n",
    "\n",
    "        loss_d_, _ = sess.run([D_loss, D_optim], {x: x_, z: z_, isTrain: True, lda: lda_})\n",
    "        D_losses.append(loss_d_)\n",
    "\n",
    "        # update generator\n",
    "        z_ = np.random.normal(0, 1, (batch_size, 1, 1, 100))\n",
    "                \n",
    "        # generate a batch of images\n",
    "        G_z_samples = sess.run(G_z, feed_dict={z: z_, isTrain: True})\n",
    "   \n",
    "        # np functions to compute score\n",
    "        scores = []\n",
    "        minValues = []\n",
    "        maxValues = []\n",
    "        for i in range(batch_size):\n",
    "\n",
    "            score, minValue, maxValue = compute_score_np(G_z_samples[i])\n",
    "\n",
    "            scores.append(score)\n",
    "            minValues.append(minValue)\n",
    "            maxValues.append(maxValue)\n",
    "\n",
    "        scores = np.expand_dims(np.array(scores), axis=1)\n",
    "        minValues = np.expand_dims(np.array(minValues), axis=1)\n",
    "        maxValues = np.expand_dims(np.array(maxValues), axis=1)\n",
    "        fd = {x: x_, z: z_, isTrain: True, lda: lda_, scores_1: scores, mins_1: minValues, maxs_1: maxValues}\n",
    "        loss_g_, scores_g_, mins_g_, maxs_g_, _ = sess.run([G_loss, scores_G, mins_G, maxs_G, G_optim], feed_dict=fd)\n",
    "        G_losses.append(loss_g_)\n",
    "        G_scores.append(scores_g_)\n",
    "        G_mins.append(mins_g_)\n",
    "        G_maxs.append(maxs_g_)\n",
    "\n",
    "    epoch_end_time = time.time()\n",
    "    per_epoch_ptime = epoch_end_time - epoch_start_time\n",
    "    print('[%d/%d] - ptime: %.2f loss_d: %.3f, loss_g: %.3f, scores: %.3f, mins: %.3f, maxs: %.3f' % ((epoch + 1), train_epoch, per_epoch_ptime, np.mean(D_losses), np.mean(G_losses), np.mean(G_scores), np.mean(G_mins), np.mean(G_maxs)))\n",
    "    fixed_p = root + pngLocation + '/' + model + str(epoch + 1) + '.png'\n",
    "    show_result((epoch + 1), save=True, path=fixed_p)\n",
    "    train_hist['D_losses'].append(np.mean(D_losses))\n",
    "    train_hist['G_losses'].append(np.mean(G_losses))\n",
    "    train_hist['per_epoch_ptimes'].append(per_epoch_ptime)\n",
    "\n",
    "end_time = time.time()\n",
    "total_ptime = end_time - start_time\n",
    "train_hist['total_ptime'].append(total_ptime)\n",
    "\n",
    "print('Avg per epoch ptime: %.2f, total %d epochs ptime: %.2f' % (np.mean(train_hist['per_epoch_ptimes']), train_epoch, total_ptime))\n",
    "print(\"Training finish!... save training results\")\n",
    "with open(root + model + 'train_hist.pkl', 'wb') as f:\n",
    "    pickle.dump(train_hist, f)\n",
    "\n",
    "show_train_hist(train_hist, save=True, path=root + model + 'train_hist.png')\n",
    "\n",
    "images = []\n",
    "for e in range(train_epoch):\n",
    "    img_name = root + pngLocation + '/' + model + str(e + 1) + '.png'\n",
    "    images.append(imageio.imread(img_name))\n",
    "imageio.mimsave(root + model + 'generation_animation.gif', images, fps=5)\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
